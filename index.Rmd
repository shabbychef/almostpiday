---
title       : Backtesting
subtitle    : war stories and cautionary tales.
author      : Steven E. Pav
job         : (former quant)
framework   : revealjs      # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax, bootstrap]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
revealjs    : 
  transition  : "none"
  center      : "false"
  transitionSpeed : "fast"
  theme : "sky"    
---

<!-- zoom transition also good, but a little distracting -->
<!-- c.f.  
http://zevross.com/blog/2014/11/19/creating-elegant-html-presentations-that-feature-r-code/
http://stackoverflow.com/a/21468200/164611
https://github.com/hakimel/reveal.js/
https://j.eremy.net/align-lists-flush-left/
-->
 
<style type="text/css">
p { text-align: left; }
</style>

### My background

* Former applied mathematician. (IUB graduate!)
* Quant Programmer & Quant Strategist 2007-2015 at 
two small hedge funds.
* Almost pure quant funds, ML-based, in U.S. ("single name") equities and
volatility futures.
* Tried many ML technologies to construct strategies: SVM, random forests, 
GP.
* Also traditional statistical approaches: plain old regression.
* Whatever the approach, we used _backtests_. (GP used them heavily.)

--- .class #backtests

#### What makes a profitable strategy?

<div align=left>
<ul>
<li> Need prediction of future price movements.
</ul>
</div>

But also:

* Turn the predictions into trades.
* No, _really_, turn the predictions into trades.
* Eliminate or reduce exposure to certain risks.
* Control trade costs. (commissions, short financing, market impact.)

Hard to estimate the effects of the different moving parts separately.

So simulate your trading historically: A _backtest._

Backtesting basically implies quantitative strategies: you cannot backtest
discretionary trading.

--- .class #whatdo

### Backtests

A backtest probably should:

* simulate the effects of your actions (orders submitted).
* simulate the actions of the world (fills, commissions, corporate actions,
_etc_.).
* translate in an obvious way to a real trading strategy.
* provide an absolute guarantee of _time safety_.

Creating a good backtesting environment requires:

* Software engineering: balance _time safety_, _computational efficiency_
and _developer sanity_.
* Domain knowledge and data: _How should you simulate fill?_
_How do corporate actions work?_
* Great statistical powers: _How do you interpret the results?_ _How do you
avoid overfitting?_
* Good intuition and sleuthing abilities: _What new thing is broken?_

--- .class #kindsof

### Different kinds of backtests

<center>![](./figure/backtests.png)</center>


--- .class #garbatrage

<style type="text/css">
p { text-align: left; }
</style>

### Garbatrage

Use Bayes' Rule:

* Devising a consistently profitable trading strategy is known to be hard. <br>
(The EMH posits that it is essentially _impossible_.)
* Bugs are easy to make. A good programmer will make several a day.

If your backtest looks profitable, what is the likelihood the
strategy is really profitable?

If you are exploring a new asset class, using a new fill simulator, 
using new code, testing a new strategy, or reading a paper from a third party, 
and the backtest looks great, _it's probably a bug_.

--- .class #anexample

<style type="text/css">
p { text-align: center; } 
</style>

### An example

Should three day old tweets give you this?

```{r bmzsim,echo=FALSE,warning=FALSE,fig.width=14.0,fig.height=7,out.width="900px",out.height="500px"}
rvs <- read.csv('data/YAHOO-INDEX_DJI.csv',stringsAsFactors=FALSE)
# flip up down
rvs[,1] <- as.Date(rvs[,1])
rvs <- rvs[seq(nrow(rvs),1,by=-1),]

ac <- rvs[,ncol(rvs)]
rrs <- diff(ac,1) / ac[1:(length(ac)-1)]
pwin <- 13/15
dat <- as.Date(rvs[,1])[-1]
set.seed(12345)
wins <- 2 * rbinom(length(rrs),1,prob=pwin) - 1
rrwin <- abs(rrs) * wins
llwin <- log(1 + rrwin)
mtmwin <- exp(cumsum(llwin))
mtm <- data.frame(date=dat,mtm=mtmwin)
require(ggplot2)
ph <- ggplot(mtm,aes(x=date,y=mtm)) + 
	geom_line(size=1) + 
	scale_y_log10() 
print(ph)
#library(SharpeR)
#print(as.sr(llwin,ope=252))
```

<style type="text/css">
p { text-align: left; }
</style>

--- .class #timetravelzero

### Time Travel

The most common error in backtests is _time travel_: use of future information 
in simulations.

Time travel occurs for many reasons:

* Backfill and survivorship bias.
* Representation of corporate actions: dividends, splits, spinoffs, mergers,
warrants.
* Think-os and code boo boos.

Time travel is easy to simulate, but hard to implement!

--- .class #survivorship

### Survivorship and Backfill

* Classic survivorship bias: trading on a universe of stocks
defined by _present_ membership in some index, say.
* Data vendors often backfill data for companies or remove them.
(You can test for this, or just ask them!)
* Vendors may do weird things to deal with mergers (or you may!)

--- .class #corporate

### Corporate Actions 

* Corporate actions are notoriously time-leaky.
* Problems stem from representing asset returns as a single time series: in reality, they
_branch_ across time.
* Corporate actions are just 
[hard to model](http://www.denisonmines.com/s/Corporate_History.asp).

For example, (back) adjusted closes. Investing inversely proportional to
adjusted close gives a time-travel 'arb'.

```{r aapl,echo=FALSE,warning=FALSE,fig.width=10.0,fig.height=4.0,out.width="900px",out.height="400px"}
library(tidyr)
library(plyr)
msft <- read.csv('data/YAHOO-MSFT.csv',stringsAsFactors=FALSE)
# flip up down
msft[,1] <- as.Date(msft[,1])
msft <- msft[seq(nrow(msft),1,by=-1),]

allem <- msft %>%
	tidyr::gather(key=mark,value=price,-Date)

library(ggplot2)
ph <- ggplot(allem %>% dplyr::filter(mark %in% c('Close','Adjusted.Close')),
	aes(x=Date,y=price,group=mark,colour=mark)) + 
	geom_line(size=1) + 
	scale_y_log10() +
  labs(title='MSFT')
print(ph)
```

--- .class #mlhackerone

### The ML Hacker Trap

* Align returns to features for training ML models.
* Forget that the model is timestamped to the _returns_.
* A warning: _the more often I retrain, the better
my model!_
(Often with an excuse for 'time freshness'.)

<center>![](./figure/align1.png)</center>

--- .class #mlhackertwo

### The ML Hacker Trap

* Align returns to features for training ML models.
* Forget that the model is timestamped to the _returns_.
* A warning: _the more often I retrain, the better
my model!_
(Often with an excuse for 'time freshness'.)

<center>![](./figure/align2.png)</center>

--- .class #break

## Break!

Time permitting, talk about overfitting later.

Turn it over to Zak for the next part.

--- .class #ohno

### Again!?

<center>![](./figure/Curve_fitting.jpg)</center>

--- .class #poo_one

## Plain Old Overfitting

When backtests are fixed, you can move on to overfitting.

Overfitting has two flavors (to me):

1. Building a decent model for the assets and features used, but
overestimating future performance based on in-sample data.
(Using same data to build model and estimate it's Sharpe.)
2. Adding opportunities [decreases the expected Sharpe](http://arxiv.org/abs/1409.5936)
due to the added degrees of freedom.

A Quant's Kōan:

> Ignore data and your model is suboptimal.
> Use all the data: your model is
> suboptimal.

--- .class #poo_two

## Plain Old Overfitting

Overfit is _not_ cured by silly ML tricks:

You can talk about hold-out sets and in-sample all you want. 
_There is no out-of-sample._ There is _in-sample_ and 
_trading-real-money-on-the-strategy_.

Overfit is approached as a technical problem, but fighting
it requires discipline and soft engineering skills:
If you don't keep track of everything you have tried, you can't 
apply most techniques.

--- .class #terminology

## Some Terminology

As an approximation, suppose that the returns of an asset (a strategy, stock,
ETF, whatever) are constant over time.

Typically use $\mu$ for the expected value, and $\sigma^2$ for the variance
of the returns.

Informally, the Sharpe ratio is defined as
$$
\zeta = \frac{\mu - r_0}{\sigma},
$$
where $r_0$ is the 'risk-free' or 'disastrous rate' of return.

(Formally, I tend to use 'Sharpe' to refer to a sample statistic, and
'signal-noise-ratio' to refer to population quantities. Let's not be
so pedantic here.)

--- .class #twoproblems

## Technical Approaches

Two forms of the problem I was interested in:

1. For automated strategy search: having observed the in-sample Sharpe of $n \gg 10^4$ strategies,
possibly selected by hill-climbing in-sample Sharpe, could we de-bias the Sharpe of the optimal one? 
Or could we select some top $k$ of them and 'average' the strategies?
2. For human strategy search: having observed the _returns_ of $n \ge 10^3$ strategies, along with
the settings of some 'knobs' for each of them, could we estimate the effects of
each knob? Could we pick the best knob setting and de-bias the estimated future
Sharpe?

I suspected classical approaches (WRC and extensions, Hansen's SPA, _etc._)
would not work: different input, wrong assumptions, wouldn't scale.

--- .class #poapprox

### Optimal Sharpe over many correlated strategies

Suppose you observed the time series of returns of $n \gg 10^4$ strategies, each
of length $T$, in $T \times n$ matrix $X$.

Think of a dimensionality reduction, where $X$ is nearly contained in some $k$
dimensional subspace, with $k \ll T$:
$$
X \approx L W,
$$
where $L$ is $T \times k$, and $W$ is $k \times n$.

You can think of returns in $X$ as portfolios over latent returns in $L$ with
portfolio weights $W$. 

What is the maximal Sharpe over $k$ assets?

--- .class #poapproxtwo

### Optimal portfolio Sharpe

Optimal Sharpe over $k$ assets is achieved by the Markowitz portfolio.
Distribution is related to Hotelling's $T^2$.

As an aside, interesting connections between quant metrics 
(the 'right' ones) and classical statistics:

 quant world  |  classical statistics  
--------------|-------------------------
Sharpe ratio                   | $t$ statistic 
squared Sharpe of Markowitz Portfolio | Hotelling $T^2$ 
expected squared Sharpe, conditional on features | Hotelling-Lawley Trace

--- .class #poapproxthree

### Optimal portfolio Sharpe

Optimal Sharpe over $k$ assets is achieved by the Markowitz portfolio.
Distribution is related to Hotelling's $T^2$.

We know the distribution of the (in-sample) Sharpe of the Markowitz portfolio
as a function of $k$, $T$, and the (population) Sharpe of the 
(population) Markowitz portfolio. 

We also have good estimators of, and confidence intervals around that population 
optimal Sharpe given the in-sample statistics. (Given in
[SharpeR](http://github.com/shabbychef/SharpeR) for example.)

Note that the $X$ did not need to be observed, only the optimal Sharpe over
$X$. (Downside: have to estimate $k$)

--- .class #sharpeeffectsone

### Human overfitting problem

Observe time series of returns of $n \approx 10^3$ strategies, each
of length $T$, in $T \times n$ matrix $X$. Also observe $f \approx 50$
'features' on each asset in $f \times n$ matrix $F$.

Modeling the Sharpe of each asset as some _linear_ function of the features in
$F$. If $\mathbf{\zeta}$ is the $n$-vector of Sharpes, we have
$$
\mathbf{\zeta} \approx F^{\top} \beta
$$
for some unknown $f$-vector $\beta$.

--- .class #sharpeeffectstwo

### Human overfitting problem

When $n$ is reasonable, from $X$ you can compute the vector of sample Sharpes
$\mathbf{\zeta}$ and the $n\times n$ matrix of variance-covariance,
$\Omega$.
(Classical asymptotic result of Jobson & Korkie.)

Then regress $\mathbf{\zeta}$ against $F^{\top}$, and the variance-covariance
around estimated $\beta$ is
$$
\left(F^{\top}F\right)^{-1} F^{\top} \Omega F \left(F^{\top}F\right)^{-1}
$$

Result is estimate of the _effects_ of each feature and variance-covariance
around them.
